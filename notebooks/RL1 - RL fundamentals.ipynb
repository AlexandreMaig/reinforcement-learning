{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/reinforcement-learning/\">https://supaerodatascience.github.io/reinforcement-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Class 1: Reinforcement Learning fundamentals</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this course works (pedagogically):\n",
    "- one notebook to rule them all (them = the concepts)\n",
    "- no slides\n",
    "- short exercices along the way\n",
    "- a bit of live coding\n",
    "- two class breaks for you to breathe\n",
    "    \n",
    "What you should expect:\n",
    "- some plain words notions,\n",
    "- but avoidance of over-simplification,\n",
    "- and also a fair amount of (hopefully painless) rigorous notations and abstract concepts.\n",
    "- Also most things will be fully written down to increase your autonomy in replaying the notebook.\n",
    "\n",
    "Duration:  \n",
    "This notebook was played in the [introductory course](https://rlvs.aniti.fr/rl-fundamentals.html) of the 2021 [Reinforcement Learning Virtual School](https://rlvs.aniti.fr) (video class available). The class lasted 3.5 hours (with breaks).  \n",
    "For an in-class experience, it is safer to reserve 6 hours.\n",
    "\n",
    "Color code:\n",
    "<div class=\"alert alert-success\">Key results in green boxes</div>\n",
    "<div class=\"alert alert-warning\">Exercices in yellow boxes</div>\n",
    "<div class=\"alert alert-danger\">Solutions in red boxes</div>\n",
    "\n",
    "And a first yellow box:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic algebra.\n",
    "- Random variables, probability distributions.\n",
    "- Gradient descent.\n",
    "    \n",
    "**Useful but not compulsory:**\n",
    "- Random processes, Markov chains.\n",
    "- Notion of contraction mapping.\n",
    "- Dynamic Programming\n",
    "- Stochastic Gradient Descent.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "<li>\n",
    "    <span><a href=\"#Class-goals\" data-toc-modified-id=\"Class-goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Class goals</a></span>\n",
    "</li>\n",
    "<li>\n",
    "    <span><a href=\"#Ruining-the-suspense-with-a-general-abstract-definition\"\n",
    "            data-toc-modified-id=\"Ruining-the-suspense-with-a-general-abstract-definition-2\"\n",
    "            ><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ruining the suspense with a general abstract\n",
    "            definition</a></span>\n",
    "</li>\n",
    "<li>\n",
    "    <span><a href=\"#RL-within-Machine-Learning\"\n",
    "            data-toc-modified-id=\"RL-within-Machine-Learning-3\"\n",
    "            ><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>RL within Machine Learning</a></span>\n",
    "</li>\n",
    "<li>\n",
    "    <span><a href=\"#From-plain-words-to-first-variables\"\n",
    "            data-toc-modified-id=\"From-plain-words-to-first-variables-4\"\n",
    "            ><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>From plain words to first variables</a></span>\n",
    "</li>\n",
    "<li>\n",
    "    <span><a href=\"#Modeling-sequential-decision-problems-with-Markov-Decision-Processes\"\n",
    "            data-toc-modified-id=\"Modeling-sequential-decision-problems-with-Markov-Decision-Processes-5\"\n",
    "            ><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling sequential decision problems with Markov\n",
    "            Decision Processes</a></span>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span><a href=\"#Definition\" data-toc-modified-id=\"Definition-5.1\"\n",
    "                    ><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Definition</a></span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span><a href=\"#Value-of-a-trajectory-/-of-a-policy\"\n",
    "                    data-toc-modified-id=\"Value-of-a-trajectory-/-of-a-policy-5.2\"\n",
    "                    ><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Value of a trajectory / of a policy</a></span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span><a href=\"#Optimal-policies\" data-toc-modified-id=\"Optimal-policies-5.3\"\n",
    "                    ><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Optimal policies</a></span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.4\"\n",
    "                    ><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Summary</a></span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span><a href=\"#Homework:-MDP-notions\" data-toc-modified-id=\"Homework:-MDP-notions-5.5\"\n",
    "                    ><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Homework: MDP notions</a></span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- acquire the fundamental building blocks of RL:\n",
    "    - plain word notions\n",
    "    - MDPs, policies, optimality equations, etc.\n",
    "    - common notations\n",
    "    - key algorithms\n",
    "    - common misconceptions\n",
    "- key challenges in RL and their connection to future lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ruining the suspense with a general abstract definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Reinforcement Learning about?\n",
    "\n",
    "It is about learning to control dynamic systems.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"width: 400px;\"></img></center>\n",
    "\n",
    "Dynamic systems? **dynamic** evolution of $s$ and $o$ under $\\pi$ over a certain time horizon.\n",
    "\n",
    "Our object of study:<br>\n",
    "We want to find a control policy $\\pi$ (with $u = \\pi(o)$) such that the system $\\Sigma$ behaves as we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of RL problems <a class=\"tocSkip\">\n",
    "\n",
    "Controler un avion : on a des état avec les alitmètre, la vitesse, ... et quand on utilise les gouverne ca change\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td><img src=\"img/spiral.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Exiting a spiral</td>\n",
    "  <td><img src=\"img/tests.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Dynamic treatment regimes for HIV patients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/pend.png\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Cart-pole balancing</td>\n",
    "  <td><img src=\"img/waiting.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Queueing problems</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td><img src=\"img/market.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td style=\"border-right:1px solid;\">Portfolio management</td>\n",
    "  <td><img src=\"img/dam.jpg\" style=\"width: 200px;\"></td>\n",
    "  <td>Hydroelectric production</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "But also:\n",
    "- Elevator scheduling\n",
    "- Bicyle riding\n",
    "- Ship steering\n",
    "- Bioreactor control\n",
    "- Aerobatics helicopter control\n",
    "- Airport departures scheduling\n",
    "- Ecosystem regulation and preservation\n",
    "- Robocup soccer\n",
    "- Video game playing (Atari, Starcraft...)\n",
    "- Game of Go\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, learning to play a board game, learning to juggle, learning to take good strategic decisions, learning to drive... all fall into the same category of **control problems** and Reinforcement Learning studies the process of **elaborating a good control strategy through interaction samples**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "Reinforcement Learning is about learning an optimal sequential behavior in a given environment.\n",
    "</div>\n",
    "\n",
    "Let's break this down.\n",
    "- sequential behavior in a given environment  \n",
    "$\\rightarrow$ discrete time steps, sequence of actions\n",
    "- optimal  \n",
    "$\\rightarrow$ a reward signal informs us of the quality of the last action\n",
    "- learning  \n",
    "$\\rightarrow$ no known model, just interaction samples, behavior adaptation.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"width: 400px;\"></img></center>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Keywords:**\n",
    "- system to control / environment\n",
    "- control policy\n",
    "- optimality\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On discrétise en pas de temps. On cherche un état  optimale et on défini une fonction de récompense qui permet de dire si l'état dans lequel on est bonne ou non."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Warm-up poll:** \n",
    "How do you do today?  \n",
    "[https://linkto.run/p/BOOR15YA](https://linkto.run/p/BOOR15YA)\n",
    "- Great, I'm learning RL!\n",
    "- Great, but I'm scared the RL unicorn will turn into a difficult to tame rhino.\n",
    "- Great, bring the math on (as long as you do it step by step).\n",
    "- Why do you ask the question if the only answer is \"Great\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standing on the shoulders of giants**\n",
    "\n",
    "> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Sutton & Barto, 2018, [Reinforcement Learning: an Introduction](http://incompleteideas.net/book/the-book-2nd.html))\n",
    "\n",
    "Caveat: this is a definition of *learning*, not specifically of *reinforcement learning* (although it applies to RL), so it is worth giving some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL within Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have had classes on Machine Learning before. There are three strongly distinct categories of problems in ML:\n",
    "- Supervised Learning\n",
    "- Unsupervised Learning\n",
    "- Reinforcement Learning\n",
    "\n",
    "Let's try to answer the following questions for each category.\n",
    "- What's the abstract problem we are trying to solve?\n",
    "- What's the data provided to the algorithms?\n",
    "- Give examples of algorithms in SL/UL/RL.  \n",
    "\n",
    "<center>\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "    <td> <b>Question</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Supervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Unsupervised</b> </td>\n",
    "    <td style=\"border-left: 1px solid black\"> <b>Reinforcement</b> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $ f(x)=y $ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $ x\\in X $ </td>\n",
    "    <td style=\"border-left: 1px solid black\"> $ \\pi(s)=a $ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Target (rephrased) </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Predict outputs given inputs</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Discover structure in data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Find an optimal behavior </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> {(x,y)} supervisor's labels </td>\n",
    "    <td style=\"border-left: 1px solid black\"> {x} unlabelled data </td>\n",
    "    <td style=\"border-left: 1px solid black\"> {(s,a,r,s')} experience samples </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Output </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Classifier or regressor</td>\n",
    "    <td style=\"border-left: 1px solid black\"> Clusters or dimension reduction </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Policies, value functions </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td> Key algorithms </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Neural networks, SVMs, etc.</td>\n",
    "    <td style=\"border-left: 1px solid black\"> k-means, PCA, etc. </td>\n",
    "    <td style=\"border-left: 1px solid black\"> Q-learning, Policy Gradients, etc. </td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "This table helps distinguish the different natures of the problems tackled. The RL problem is about finding the optimal policy for a given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A un état donnée s, on a fait une action a, qui nous donne une récompense r et  cela donne une nouvel éta s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this different from Supervised Learning?\n",
    "- no correct $(s,a)$ example, rather $(s,a,r,s')$ samples\n",
    "- Delayed rewards, credit assignment, trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:** Pick the true statement(s).  \n",
    "[https://linkto.run/p/3OG3IJO3](https://linkto.run/p/3OG3IJO3)\n",
    "- Sorting new emails as spam (or not) given a million labelled emails is a reinforcement learning task.\n",
    "- Deciding what move to play at chess, based on thousands of previous games is a reinforcement learning task.\n",
    "- Incrementally improving the accuracy of a radar detection software from online collected data is a reinforcement learning task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspirations for RL:\n",
    "- Control theory and Stochastic processes for the **modeling** part\n",
    "- Statistics, Optimization and Cognitive Psychology for the **learning** part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From plain words to first variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A medical prescription example <a class=\"tocSkip\">\n",
    "\n",
    "<center><img src=\"img/patient-doctor.png\" style=\"height: 200px;\"></center>\n",
    "    \n",
    "A patient walks into a clinic with her medical file (medical history, x-rays, blood work, etc.). You, as her doctor, need to write a prescription. Let us use this example to formalize the process of deciding what to write on the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Patient variables <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<div style=\"background-color:white; color:black; padding:15px;\">\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "</div>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states:$S_t$\n",
    "</center>\n",
    "\n",
    "The medical file of the patient allows us to define a number of variables that characterize the patient now. We will write $S_0$ the vector of these variables. Future measurements will be noted $S_t$.\n",
    "\n",
    "$S_t$ is a random vector, taking different values in a *patient description space* $S$ at different time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prescription <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<div style=\"background-color:white; color:black; padding:15px;\">\n",
    "\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "</div>\n",
    "\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>\n",
    "\n",
    "The prescription is a series of recommendations we give to the patient over the course of treatment. It is thus a sequence $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$ of variables $A_t$.\n",
    "\n",
    "These treatments $A_t$ are random variables too, taking their value in some space $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient evolution <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "<center>\n",
    "<div style=\"background-color:white; color:black; padding:15px;\">\n",
    "\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</div>\n",
    "\n",
    "</center>\n",
    "\n",
    "The patient evolves over time steps. Her evolution follows a certain probability distribution $\\mathbb{P}(S_t)$ over descriptive states.\n",
    "\n",
    "So $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ defines a *random process* that describes the patient's evolution under the influence of past $S_t$ and $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physician's goal <a class=\"tocSkip\">\n",
    "\n",
    "<center>\n",
    "<div style=\"background-color:white; color:black; padding:15px;\">\n",
    "<img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br>\n",
    "\n",
    "</div>\n",
    "\n",
    "</center>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$\n",
    "\n",
    "The physician's goal is to bring the patient from an unhealthy state $S_0$ to a healthy situation.  \n",
    "\n",
    "This goal is not only defined by a final state of the patient but by the full trajectory followed by the variables $S_t$ and $A_t$. For example, prescribing a drug that damages the patient's liver, or letting the patient experience too much pain over the course of treatment is discouraged.\n",
    "\n",
    "We define a criterion $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$ that allows to quantify how good a trajectory in the joint $S\\times A$ space is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up <a class=\"tocSkip\">\n",
    "\n",
    "- Patient state $S_t$  (random variable)\n",
    "- Physician instruction $A_t$ (random variable)\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$   \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$  \n",
    "- Patient's trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process\n",
    "- Value of a trajectory $J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common misconception <a class=\"tocSkip\">\n",
    "\n",
    "You will often see the following type of drawing, along with a sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<center><img src=\"img/misconception.png\" style=\"height: 300px;\"></img></center>\n",
    "\n",
    "Although this sentence is not false *per se*, it conveys an important misconception that may be grounded in too simple anthropomorphic analogies. One often talks about the *state of the agent* or the *state of the environment*. The distinction here is confusing at best: there is no separation between agent and environment. A better vocabulary is to talk about a *system to control*, that is described through its observed *state*. This system is controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with.\n",
    "\n",
    "Although less shiny, the drawing below may be less misleading.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"height: 300px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Three key notions <a class=\"tocSkip\">\n",
    "\n",
    "Understanding RL is a three-stage rocket, answering the questions:  \n",
    "1. What is the system to control?  \n",
    "2. What is an optimal strategy?  \n",
    "3. How do we learn such a strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise (no poll):**  \n",
    "Suppose that, instead of treating a patient, we want to learn to swing the pole up in the cart-pole example.  \n",
    "What are the state description variables?  \n",
    "What are the action variables?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pend.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "State: cart position and velocity $x, \\dot{x}$, pole angle and velocity $\\theta, \\dot{\\theta}$.\n",
    "    \n",
    "Action: force $F$ applied on the cart.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Modeling sequential decision problems with Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Definition\n",
    "\n",
    "Let's take a higher view and develop a general theory for describing problems such as writing a prescription for our patient.\n",
    "\n",
    "Let us assume we have:\n",
    "- a set of states $S$ describing the system to control,\n",
    "- a set of actions $A$ we can apply.\n",
    "\n",
    "Curing patients is a conceptually difficult task. \n",
    "To keep things grounded, we shall use a toy example called [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) and work our way to more general concepts. It's also the occasion to familiarize with [OpenAI Gym](https://gym.openai.com/).\n",
    "\n",
    "<center><img src=\"img/frisbee.jpg\" style=\"height: 300px;\"></img></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\") # use render_mode=\"human\" to open the game window\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful only if you have used render_mode=\"human\" in the cell above\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The game's goal is to navigate across this lake, from position S to position G, in order to retrieve a frisbee, while avoiding falling into the holes H. Frozen positions are slippery so you don't always move in the intended direction. Reaching the goal provides a reward of 1, and zero otherwise. Falling into a hole or reaching the goal ends an episode.\n",
    "\n",
    "Take a look at the funny description in `help(fl.FrozenLakeEnv)` if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Poll:**  \n",
    "[https://linkto.run/p/65E9EO4Q](https://linkto.run/p/65E9EO4Q)  \n",
    "How many states are there in this game?  \n",
    "How many actions?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "States set: the 16 positions on the map.  \n",
    "Actions set: the 4 actions $\\{$N,S,E,W$\\}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's confirm that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At every time step, the system state is $S_t$ and we decide to apply action $A_t$. This results in observing a new state $S_{t+1}$ and receiving a scalar reward signal $R_t$ for this transition.\n",
    "\n",
    "$R_t$ tells us how happy we are with the last transition.\n",
    "\n",
    "For example, in FrozenLake, all transitions have reward 0 except for the one that reaches the goal, which yields reward 1. Let's verify this and introduce a few utility functions on the way.\n",
    "\n",
    "Note that $S_t$, $A_t$, $S_{t+1}$ and $R_t$ are random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
      "Apply → from (3, 2):\n",
      "  Reach ((3, 2)) and get reward 0 with proba 0.33333333333333337.\n",
      "  Reach ((3, 3)) and get reward 1 with proba 0.3333333333333333.\n",
      "  Reach ((2, 2)) and get reward 0 with proba 0.33333333333333337.\n"
     ]
    }
   ],
   "source": [
    "actions = {fl.LEFT: \"\\u2190\", fl.DOWN: \"\\u2193\", fl.RIGHT: \"\\u2192\", fl.UP: \"\\u2191\"}\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row * env.unwrapped.ncol + col\n",
    "\n",
    "def to_row_col(s):\n",
    "    col = s % env.unwrapped.ncol\n",
    "    row = int((s - col) / env.unwrapped.ncol)\n",
    "    return row, col\n",
    "\n",
    "print(actions)\n",
    "row = 3\n",
    "col = 2\n",
    "a = 2\n",
    "print(f\"Apply {actions[2]} from ({row}, {col}):\")\n",
    "for tr in env.unwrapped.P[to_s(row,col)][a]:\n",
    "    print(f\"  Reach ({to_row_col(tr[1])}) and get reward {tr[2]} with proba {tr[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov = proba de passer a un niveau état t+1 ne dépend que de l'état précédent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now make our main assumption about the systems we want to control.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Fundamental assumption (Markov property)**\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t, S_{t-1}, A_{t-1}, \\ldots, S_0, A_0) = \\mathbb{P}(S_{t+1},R_t|S_t, A_t)$$\n",
    "</div>\n",
    "    \n",
    "Such a system will be called a Markov Decision Process (MDP).\n",
    "\n",
    "One generally separates the state dynamics and the rewards by:\n",
    "$$\\mathbb{P}(S_{t+1},R_t|S_t, A_t) = \\mathbb{P}(S_{t+1}|S_t, A_t)\\cdot \\mathbb{P}(R_t|S_t, A_t, S_{t+1})$$\n",
    "\n",
    "Which leads in turn to the general definition of an MDP:\n",
    "<div class=\"alert alert-success\"><b>Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A set of states $S$\n",
    "<li> A set of actions $A$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "\n",
    "A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "\n",
    "A set of discrete decision epochs $T=\\{0,1,\\ldots,H\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Most of the results presented here can be found in M. L. Puterman's classic book, [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9781118625873).\n",
    "\n",
    "If $H\\rightarrow\\infty$ we have an infinite horizon control problem.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Since we will only work with infinite horizon problems, we shall identify the MDP with the 4-tuple $\\langle S,A,p,r\\rangle$.\n",
    "</div>\n",
    "    \n",
    "So, in RL, we wish to control the trajectory of a system that, we suppose, behaves as a Markov Decision Process.\n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"height: 240px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Value of a trajectory / of a policy\n",
    "\n",
    "An oracle decides on how to choose actions at each time step:\n",
    "$$A_t \\sim \\pi_t.$$\n",
    "\n",
    "$\\pi_t$ is called the **decision rule** at step $t$, it is a distribution over the action space $A$.  \n",
    "The collection $\\pi = \\left(\\pi_t \\right)_{t\\in\\mathbb{N}}$ is the oracle's **policy**.\n",
    "\n",
    "<center><img src=\"img/frisbee.jpg\" style=\"height: 100px;\"></img></center>\n",
    "\n",
    "One policy implies one specific distribution over trajectories over the frozen lake. More generally, the policy and $S_0$ condition the sequence $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$\n",
    "\n",
    "In FrozenLake as in the patient's example, some trajectories are better than others. We need a criterion to compare trajectories. Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
    "\n",
    "Let's compare the policy that always moves to the right and the policy that always moves left by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 30915.09it/s]\n",
      "100%|██████████| 50000/50000 [00:04<00:00, 10606.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est. value of 'right' policy: 0.03112 variance: 0.17364200413494427\n",
      "est. value of 'left'  policy: 0.0 variance: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "nb_episodes = 50000\n",
    "horizon = 200\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"ansi\")\n",
    "\n",
    "Vright = np.zeros(nb_episodes)\n",
    "for i in trange(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, _, _ = env.step(fl.RIGHT)\n",
    "        Vright[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "Vleft  = np.zeros(nb_episodes)\n",
    "for i in trange(nb_episodes):\n",
    "    env.reset()\n",
    "    for t in range(horizon):\n",
    "        next_state, r, done, _, _ = env.step(fl.LEFT)\n",
    "        Vleft[i] += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"est. value of 'right' policy:\", np.mean(Vright), \"variance:\", np.std(Vright))\n",
    "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"variance:\", np.std(Vleft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que la policy aller tjrs a droite arrive parfois a l'objectif car on peut glisser. Alors que la policy aller a gauche ne fonctionne pas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "$\\gamma$ appartien a [0,1[\n",
    "\n",
    "    \n",
    "$G^\\pi(s)$ represents what we can gain in the long-term by applying the actions from $\\pi$.\n",
    "\n",
    "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
    "$$V^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
    "\n",
    "This defines the value function $V^\\pi$ of policy $\\pi$:\n",
    "<div class=\"alert alert-success\"><b>\n",
    "</div>\n",
    "\n",
    "Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "S & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "<div class=\"alert alert-success\"><b>\n",
    "</div>\n",
    "\n",
    "And, given a distribution $\\rho_0$ on starting states, we can map $\\pi$ to the scalar value:\n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} \\left[ V^\\pi(s) \\right]$$\n",
    "\n",
    "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies.\n",
    "\n",
    "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some uses the average reward criterion, and few works venture into more exotic criteria. Today, we will limit ourselves to the discounted criterion."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Optimal policies\n",
    "\n",
    "The fog clears up a bit: we can now compare policies given an initial state (or initial state distribution).  \n",
    "\n",
    "Thus, an **optimal** policy is one that is better than any other.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "<div class=\"alert alert-success\">\n",
    "</div>\n",
    "\n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in S, \\ \\forall \\pi, \\ V^{\\pi^*}(s) \\geq V^\\pi(s)$\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "</div>\n",
    "\n",
    "Note that although there may be several optimal policies, they all share the same value function $V^* = V^{\\pi^*}$."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now get to our first fundamental result. Fortunately for us...  \n",
    "\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Let's explain a little:\n",
    "- Markovian: all decision rules are only conditioned by the last seen state. Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right., \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right) = \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)$.  \n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.\n",
    "- Stationary (and Markovian): all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "- Deterministic: all decision rules put all probability mass on a single item of the action space $A$.  \n",
    "$\\pi_t(A_t|history) = \\left\\{\\begin{array}{l}\n",
    "1\\textrm{ for a single }a\\\\\n",
    "0\\textrm{ otherwise}\n",
    "\\end{array}\\right.$.\n",
    "\n",
    "So in simpler words, we know that among all possible optimal ways of picking $A_t$, at least one is a function $\\pi:S\\rightarrow A$.\n",
    "\n",
    "That helps a lot: we don't have to search for optimal policies in a complex family of history-dependent, stochastic, non-stationary policies; instead we can simply search for a function $\\pi(s)=a$ that maps states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Summary\n",
    "\n",
    "Let's wrap this whole section up. Our goal was to formally define the search for the best strategy for our game of FrozenLake and the medical prescription problem. This has led us to formalizing the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$.\n",
    "\n",
    "So we have built the first stage of our three-stage rocket:  \n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**What is the system to control?**  \n",
    "The system to control is a Markov Decision Process $\\langle S, A, p, r \\rangle$ and we will control it with a policy $\\pi:s\\mapsto a$ in order to optimize $\\mathbb{E} \\left( \\sum_t \\gamma^t R_t\\right)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Poll** The limits of MDP modeling  \n",
    "[https://linkto.run/p/0WG7WNER](https://linkto.run/p/0WG7WNER)  \n",
    "Can these systems be modeled as MDPs?   \n",
    "- Playing a tennis video game based on a single video frame -> Non car sur une frame on a pas la vitesse de la balle\n",
    "- Playing a tennis video game based on a full physical description of the ball and the players -> Oui\n",
    "- The game of Poker\n",
    "- The collaborative game of [Hanabi](https://en.wikipedia.org/wiki/Hanabi_(card_game))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "A single video frame does not contain enough information to accurately represent the current state of the game. The velocities are absent for instance. Hence the dynamics might not be Markovian.\n",
    "    \n",
    "A full physical description, however, may contain enough information so that $\\mathbb{P}(S_{t+1})$ is only conditioned by $S_t$ and $A_t$.\n",
    "    \n",
    "Poker is a two-player, adversarial, stochastic game. MDPs only model one-player games.\n",
    "\n",
    "Beyond the fact that it is a multi-player game. Hanabi is a game based mainly on epistemic reasoning. That is, reasoning on beliefs about the state of the world (specifically, the state of the other players' hand). This type of state description is difficult to encode within a Markovian dynamics model.\n",
    "\n",
    "**A bit of additional discussion to generalize these notions:**\n",
    "\n",
    "What if the system is an MDP but its state is not fully observable?  \n",
    "$\\rightarrow$ This is the (exciting) field of Partially Observable MDPs. Our key result of having a Markovian optimal policy does not hold anymore. There are ways to still obtain optimal policies (but it is often very computationaly costly) or approximate them with Markovian policies.\n",
    "\n",
    "What happens if there are multiple actions taken at the same time by different agents?  \n",
    "$\\rightarrow$ This falls into the category of multi-player stochastic games. Such games can be adversarial, cooperative, or a mix of the two. Of course they can also have partial observability.\n",
    "\n",
    "What if the transition model is not Markovian?  \n",
    "$\\rightarrow$ Beware, here be dragons! All the beautiful framework above crumbles down if its hypothesis are violated. So great care should be taken when choosing the state variables for a given problem. In a sense, an MDP is a discrete time version of a first-order differential equation. Writing a system as $\\dot{X} = f(X,U, noise)$ as is common in Control Theory is a good practice to ensure the Markov property.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Let's take a short break.**  \n",
    "**If there is time, I can take questions.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: MDP notions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise**  \n",
    "In the text above, we wrote that $\\pi_t$ is the distribution over the action space $A$ for the action $A_t$ taken at time step $t$.  \n",
    "- Write this probability $\\mathbb{P}(A_t)$ as a conditional probability $\\pi_t(A_t|\\ldots)$ (the real question is: what are the $\\ldots$?).\n",
    "- Rephrase, with your own words, what this $\\pi_t(A_t|\\ldots)$ indicates.  \n",
    "Then we defined a policy $\\pi$ as the collection of decision rules $\\left( \\pi_t \\right)_{t\\in\\mathbb{N}}$.\n",
    "- Using the answer to the previous questions, write the definition of a Markovian policy, then a stationary Markovian policy (the answer is actually in the text just after the Optimal policy theorem, the exercise is about being able to recall and explain the definitions and what they imply). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- $\\pi_t$ describes the distribution over actions at time step $t$. Because of causality (future events don't affect current events), it can only depend on the realization of the state and actions random variables in previous time steps:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t)$$\n",
    "We define the *history* $H_t = S_0, A_0, \\ldots, S_{t-1}, A_{t-1}, S_t$ at time step $t$ as this random sequence. So:\n",
    "$$\\mathbb{P}(A_t) = \\pi_t(A_t | H_t)$$\n",
    "\n",
    "- In plain words, for an action $a$ and a history $h$ at step $t$, $\\pi_t(a|h)$ indicates  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the probability to pick action $a$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; at time $t$,  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; given the history of states/actions $h$.  \n",
    "This is called a *history-dependent, non-stationary, stochastic* policy and is the most generic class of policies.\n",
    "\n",
    "- In a Markovian policy, all decision rules are only conditioned by the last encountered state.\n",
    "$$\\pi_t(A_t|H_t) = \\pi_t(A_t | S_t)$$\n",
    "In other words, given two (possibly different) sequences of state-action random variables realizations up to time $t-1$ and a single realization of S_t the distribution of $A_t$ is the same.\n",
    "Mathematically: \n",
    "$\\left\\{\\begin{array}{l}\n",
    "\\forall \\left(s_i,a_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall \\left(s'_i,a'_i\\right)_{i\\leq t-1}\\in \\left(S\\times A\\right)^{t-1}\\\\\n",
    "\\forall s \\in S\n",
    "\\end{array}\\right.,$\n",
    "\\begin{align*}\n",
    "    \\pi_t(A_t|H_t) &= \\pi_t\\left(A_t|S_0=s_0, A_0=a_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t\\left(A_t|S'_0=s'_0, A'_0=a'_0, \\ldots, S_t=s\\right)\\\\\n",
    "    &= \\pi_t(A_t | S_t)\n",
    "\\end{align*}\n",
    "One writes $\\pi_t(A_t|S_t=s)$, or more simply $\\pi_t(\\cdot | s)$.  \n",
    "In a stationary Markovian policy, all decision rules are the same throughout time. Mathematically:  \n",
    "$\\forall (t,t')\\in \\mathbb{N}^2, \\pi_t(A_t|S_t=s) = \\pi_t(A_{t'}|S_{t'}=s)$.  \n",
    "This unique distribution is written $\\pi(\\cdot | s) = \\pi_t( \\cdot | s)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "In the patient example, suppose the physician tells the patient to take drug A every day for 5 days, then drug B every two days for 9 days, then come back for a check-up. The physician adds to take drug C once a day if the patient feels pain over two consecutive days. Can you write the sequence of corresponding decision rules?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "This prescription is made over a finite horizon $H=14$ days. The actions are the combinations of drugs $A=\\left\\{ \\emptyset, (A), (B), (C), (A,B), (A,C), (B,C), (A,B,C) \\right\\}$.   \n",
    "    \n",
    "The prescription is deterministic: the distribution over actions is a Dirac. We will write it $a_t = \\pi_t(h)$.\n",
    "    \n",
    "The prescription depends on the two last states of the patient. So it's not Markovian, it is history-dependent. Precisely, it depends on the boolean state variable \"is there pain?\". So we can write $\\pi_t(h) = \\pi_t(s_t,s_{t-1})$.  \n",
    "  \n",
    "It also is not stationary, since the prescription changes after day 5.  \n",
    "    \n",
    "Consequently, the policy is:  \n",
    "For $t \\in [1, 5]$:   \n",
    "if $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (A,C)$,  \n",
    "if $pain(s_t,s_{t-1})=False$, $\\pi_t(s_t,s_{t-1}) = (A)$.  \n",
    "For $t \\in [6, 14]$:   \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (B,C)$,  \n",
    "if $t$ is even and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = (B)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=True$, $\\pi_t(s_t,s_{t-1}) = (C)$,  \n",
    "if $t$ is odd and $pain(s_t,s_{t-1})=False$,  $\\pi_t(s_t,s_{t-1}) = \\emptyset$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "Use the FrozenLake environment we've introduced earlier to obtain a Monte-Carlo estimate of $V^\\pi(s_0)$ over 100000 trials, with $s_0$ being the initial state and $\\pi$ being a simple policy that always goes right. Take $\\gamma = 0.9$. Yes, the code is almost the same as the example provided earlier.  \n",
    "Note that $\\gamma^{200} \\sim 10^{-9}$ so any reward obtained after 200 time steps will have a negligible contribution to $V^\\pi(s_0)$, thus rolling an episode out for 200 time steps should be sufficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL1_exercise1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise and note on the stationary distribution under policy $\\pi$**  \n",
    "\n",
    "Let's consider an MDP and a certain policy $\\pi$. Let's initialize the MDP to a starting state $s_0$ drawn from a distribution $\\rho_0(s)$ and let's look at how the state evolves across time steps.\n",
    "\n",
    "Because the stochastic process of $S_t$ is a Markov chain (since $\\pi$ is fixed, the probability of reaching $S_{t+1}$ is only conditionned by $S_t$), in the long run, the distribution of states follows a stationary distribution $\\rho^\\pi(s|s_0)$.\n",
    "\n",
    "This distribution is not necessarily unique: it depends on $s_0$. When all states are represented with non-zero probability in this distribution, the corresponding Markov chain is said to be *ergodic*. This is an assumption that will often be made to simplify future reasoning, even if it is false most of the time.\n",
    "\n",
    "What can we say about the stationary distribution of the Markov chain corresponding to:\n",
    "- the patient with a chronic disease under a policy that fights off the disease?\n",
    "- the patient with a deadly disease under a policy that doesn't cure her?\n",
    "- the FrozenLake example with a fixed random policy?\n",
    "- the Mad Hatter's casino (from the previous class) under a fixed random policy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "The patient with a chronic disease under a policy that fights off the disease will most likely live a rather long life (let's say infinite, for the sake of this example) and will explore states that are linked to the evolution of the disease. The states corresponding to non-recoverable situations however will not be visited.\n",
    "    \n",
    "The patient with a deadly disease and a bad treatment policy will likely die, sadly. On an infinite horizon, the stationary distribution only has probability mass on the states corresponding to death.\n",
    "    \n",
    "Similarly, the FrozenLake example has several terminal states, either by reaching the goal or by falling into a hole. It should be noted however that for such episodic environments, it is possible to define an alternate distribution $\\rho^\\pi(s|s_0)$ that describes the distribution of states before termination.\n",
    "    \n",
    "Finally, the Mad Hatter's casino under a fixed random policy is a very nice ergodic Markov chain: from any starting state there is a non-zero probability of reaching any state in a finite number of steps. No terminal states in wonderland!\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "186px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "416.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
